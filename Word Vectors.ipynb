{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50671c9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jaypr\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import json\n",
    "import string\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ensure needed packages (lightweight, idempotent)\n",
    "def _ensure_packages():\n",
    "    try:\n",
    "        import kagglehub  # noqa: F401\n",
    "    except Exception:\n",
    "        try:\n",
    "            import subprocess\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"kagglehub\"])\n",
    "        except Exception:\n",
    "            pass\n",
    "    try:\n",
    "        import gensim  # noqa: F401\n",
    "    except Exception:\n",
    "        import subprocess\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"gensim\"])\n",
    "    try:\n",
    "        import nltk  # noqa: F401\n",
    "    except Exception:\n",
    "        import subprocess\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"nltk\"])\n",
    "    try:\n",
    "        from sklearn.linear_model import LogisticRegression  # noqa: F401\n",
    "    except Exception:\n",
    "        import subprocess\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"scikit-learn\"])\n",
    "\n",
    "_ensure_packages()\n",
    "\n",
    "import gensim\n",
    "import gensim.downloader as api\n",
    "from gensim.models import Word2Vec, FastText\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "\n",
    "# NLTK data downloads\n",
    "def _ensure_nltk_data():\n",
    "    resources = [\n",
    "        (\"punkt\", \"tokenizers/punkt\"),\n",
    "        (\"punkt_tab\", \"tokenizers/punkt_tab\"),  # newer nltk sometimes needs this\n",
    "        (\"stopwords\", \"corpora/stopwords\"),\n",
    "    ]\n",
    "    for pkg, path in resources:\n",
    "        try:\n",
    "            nltk.data.find(path)\n",
    "        except LookupError:\n",
    "            nltk.download(pkg, quiet=True)\n",
    "\n",
    "_ensure_nltk_data()\n",
    "\n",
    "STOPWORDS = set(stopwords.words(\"english\"))\n",
    "RANDOM_STATE = 42\n",
    "ARTIFACT_DIR = Path(\"./artifacts\")\n",
    "ARTIFACT_DIR.mkdir(exist_ok=True, parents=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4d35a9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] Attempting to load 'word2vec-google-news-300' via gensim.downloader...\n",
      "[info] Loaded 'word2vec-google-news-300'.\n",
      "\n",
      "[PART 1] Similar words for 5 seeds\n",
      "[seeds used]: ['king', 'movie', 'love', 'computer', 'music']\n",
      "\n",
      "Top similar to 'king':\n",
      "               kings  0.7138\n",
      "               queen  0.6511\n",
      "             monarch  0.6413\n",
      "        crown_prince  0.6204\n",
      "              prince  0.6160\n",
      "              sultan  0.5865\n",
      "               ruler  0.5798\n",
      "             princes  0.5647\n",
      "        Prince_Paras  0.5433\n",
      "              throne  0.5422\n",
      "\n",
      "Top similar to 'movie':\n",
      "                film  0.8677\n",
      "              movies  0.8013\n",
      "               films  0.7363\n",
      "               moive  0.6830\n",
      "               Movie  0.6694\n",
      "        horror_flick  0.6578\n",
      "              sequel  0.6578\n",
      "Guy_Ritchie_Revolver  0.6510\n",
      "     romantic_comedy  0.6413\n",
      "               flick  0.6322\n",
      "\n",
      "Top similar to 'love':\n",
      "               loved  0.6908\n",
      "               adore  0.6817\n",
      "               loves  0.6619\n",
      "             passion  0.6101\n",
      "                hate  0.6004\n",
      "              loving  0.5887\n",
      "               Ilove  0.5703\n",
      "           affection  0.5664\n",
      "        undying_love  0.5547\n",
      "    absolutely_adore  0.5537\n",
      "\n",
      "Top similar to 'computer':\n",
      "           computers  0.7979\n",
      "              laptop  0.6640\n",
      "     laptop_computer  0.6549\n",
      "            Computer  0.6473\n",
      "           com_puter  0.6082\n",
      "technician_Leonard_Luchko  0.5663\n",
      "mainframes_minicomputers  0.5618\n",
      "    laptop_computers  0.5585\n",
      "                  PC  0.5540\n",
      "   maker_Dell_DELL.O  0.5519\n",
      "\n",
      "Top similar to 'music':\n",
      "     classical_music  0.7198\n",
      "                jazz  0.6835\n",
      "               Music  0.6596\n",
      "Without_Donny_Kirshner  0.6416\n",
      "               songs  0.6396\n",
      "           musicians  0.6336\n",
      "               tunes  0.6330\n",
      "             musical  0.6186\n",
      "         Logue_typed  0.6150\n",
      "              musics  0.6148\n",
      "\n",
      "[PART 1] Analogy tests (a - b + c)\n",
      "\n",
      "king - man + woman ≈\n",
      "               queen  0.7118\n",
      "             monarch  0.6190\n",
      "            princess  0.5902\n",
      "        crown_prince  0.5499\n",
      "              prince  0.5377\n",
      "               kings  0.5237\n",
      "       Queen_Consort  0.5236\n",
      "              queens  0.5181\n",
      "              sultan  0.5099\n",
      "            monarchy  0.5087\n",
      "\n",
      "Paris - France + Italy ≈\n",
      "               Milan  0.7222\n",
      "                Rome  0.7028\n",
      "      Palermo_Sicily  0.5968\n",
      "             Italian  0.5911\n",
      "             Tuscany  0.5633\n",
      "             Bologna  0.5608\n",
      "              Sicily  0.5596\n",
      "       Bologna_Italy  0.5470\n",
      "         Berna_Milan  0.5464\n",
      "               Genoa  0.5309\n",
      "\n",
      "actor - man + woman ≈\n",
      "             actress  0.8603\n",
      "           actresses  0.6597\n",
      "               thesp  0.6291\n",
      "             Actress  0.6165\n",
      "actress_Rachel_Weisz  0.5997\n",
      "        Best_Actress  0.5896\n",
      "              actors  0.5714\n",
      "      LIEV_SCHREIBER  0.5617\n",
      "        Oscarwinning  0.5590\n",
      "    Susan_Penhaligon  0.5583\n",
      "\n",
      "\n",
      "[info] Saved full output to: C:\\Users\\jaypr\\Desktop\\NLP_ASS\\artifacts\\part1_results.txt\n"
     ]
    }
   ],
   "source": [
    "# ==== PART 1: print all results in one go (and save to file) ====\n",
    "import os, io\n",
    "from pathlib import Path\n",
    "from contextlib import redirect_stdout\n",
    "\n",
    "import gensim\n",
    "import gensim.downloader as api\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "# --- loader (works with GoogleNews or falls back to GloVe) ---\n",
    "def load_pretrained_w2v():\n",
    "    local_path = os.environ.get(\"W2V_LOCAL_PATH\", \"\").strip()\n",
    "    if local_path and Path(local_path).exists():\n",
    "        print(f\"[info] Loading local Word2Vec model from: {local_path}\")\n",
    "        try:\n",
    "            kv = KeyedVectors.load_word2vec_format(local_path, binary=local_path.endswith(\".bin\"))\n",
    "            return kv\n",
    "        except Exception as e:\n",
    "            print(\"[warn] Failed loading local model:\", repr(e))\n",
    "\n",
    "    print(\"[info] Attempting to load 'word2vec-google-news-300' via gensim.downloader...\")\n",
    "    try:\n",
    "        kv = api.load(\"word2vec-google-news-300\")  # ~1.6GB\n",
    "        print(\"[info] Loaded 'word2vec-google-news-300'.\")\n",
    "        return kv\n",
    "    except Exception as e:\n",
    "        print(\"[warn] Could not load 'word2vec-google-news-300':\", repr(e))\n",
    "        print(\"[info] Falling back to 'glove-wiki-gigaword-100'\")\n",
    "        kv = api.load(\"glove-wiki-gigaword-100\")\n",
    "        return kv\n",
    "\n",
    "# --- helpers for robust token presence (case variations) ---\n",
    "def pick_variant_in_vocab(kv, token):\n",
    "    for v in (token, token.lower(), token.title(), token.upper()):\n",
    "        if v in kv.key_to_index:\n",
    "            return v\n",
    "    return None\n",
    "\n",
    "def safe_analogy(kv, a, b, c, topn=10):\n",
    "    aa, bb, cc = pick_variant_in_vocab(kv, a), pick_variant_in_vocab(kv, b), pick_variant_in_vocab(kv, c)\n",
    "    if None in (aa, bb, cc):\n",
    "        print(f\"\\n[skip] Missing tokens for analogy: {a}-{b}+{c} \"\n",
    "              f\"(variants: {aa or 'X'},{bb or 'X'},{cc or 'X'})\")\n",
    "        return\n",
    "    res = kv.most_similar(positive=[aa, cc], negative=[bb], topn=topn)\n",
    "    print(f\"\\n{aa} - {bb} + {cc} ≈\")\n",
    "    for term, score in res:\n",
    "        print(f\"{term:>20s}  {score:.4f}\")\n",
    "\n",
    "# --- run everything and also save full output to a file ---\n",
    "ARTIFACT_DIR = Path(\"artifacts\")\n",
    "ARTIFACT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "out_txt = ARTIFACT_DIR / \"part1_results.txt\"\n",
    "\n",
    "buf = io.StringIO()\n",
    "with redirect_stdout(buf):\n",
    "    kv = load_pretrained_w2v()\n",
    "\n",
    "    # choose 5 seeds; ensure they exist (try variants)\n",
    "    seed_candidates = [\"king\", \"movie\", \"love\", \"computer\", \"music\", \"actor\", \"woman\", \"man\", \"paris\", \"france\"]\n",
    "    seeds = []\n",
    "    for w in seed_candidates:\n",
    "        v = pick_variant_in_vocab(kv, w)\n",
    "        if v and v not in seeds:\n",
    "            seeds.append(v)\n",
    "        if len(seeds) == 5:\n",
    "            break\n",
    "\n",
    "    print(\"\\n[PART 1] Similar words for 5 seeds\")\n",
    "    print(\"[seeds used]:\", seeds)\n",
    "    for w in seeds:\n",
    "        sims = kv.most_similar(w, topn=10)\n",
    "        print(f\"\\nTop similar to '{w}':\")\n",
    "        for term, score in sims:\n",
    "            print(f\"{term:>20s}  {score:.4f}\")\n",
    "\n",
    "    print(\"\\n[PART 1] Analogy tests (a - b + c)\")\n",
    "    # use robust case-aware variants; will skip if truly missing\n",
    "    safe_analogy(kv, \"king\", \"man\", \"woman\")       # classic\n",
    "    safe_analogy(kv, \"Paris\", \"France\", \"Italy\")   # capital-country swap\n",
    "    safe_analogy(kv, \"actor\", \"man\", \"woman\")      # gendered profession\n",
    "\n",
    "# print to cell\n",
    "print(buf.getvalue())\n",
    "\n",
    "# and save full, untruncated output\n",
    "with open(out_txt, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(buf.getvalue())\n",
    "print(f\"\\n[info] Saved full output to: {out_txt.resolve()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9742a6f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[info] Using kagglehub to fetch IMDB dataset...\n",
      "[info] kagglehub path: C:\\Users\\jaypr\\.cache\\kagglehub\\datasets\\lakshmi25npathi\\imdb-dataset-of-50k-movie-reviews\\versions\\1\n",
      "[info] Loading dataset: C:\\Users\\jaypr\\.cache\\kagglehub\\datasets\\lakshmi25npathi\\imdb-dataset-of-50k-movie-reviews\\versions\\1\\IMDB Dataset.csv\n",
      "[info] Dataset size: 50000; positives=25000; negatives=25000\n",
      "\n",
      "[info] Running EDA...\n",
      "\n",
      "Token length summary:\n",
      "count    50000.000000\n",
      "mean       119.615600\n",
      "std         90.371551\n",
      "min          3.000000\n",
      "25%         64.000000\n",
      "50%         89.000000\n",
      "75%        145.000000\n",
      "max       1435.000000\n",
      "[info] Saved histogram: artifacts\\imdb_length_hist.png\n",
      "\n",
      "Top tokens (positive):\n",
      "                film  42102\n",
      "               movie  37850\n",
      "                 one  27318\n",
      "                like  17710\n",
      "                good  15025\n",
      "               great  12964\n",
      "               story  12935\n",
      "                time  12750\n",
      "                well  12724\n",
      "                 see  12274\n",
      "                also  10793\n",
      "              really  10740\n",
      "               would  10595\n",
      "                even  9616\n",
      "               first  9227\n",
      "                much  9199\n",
      "              people  8717\n",
      "                love  8692\n",
      "                best  8508\n",
      "                 get  8285\n",
      "\n",
      "Top tokens (negative):\n",
      "               movie  50114\n",
      "                film  37592\n",
      "                 one  26280\n",
      "                like  22457\n",
      "                even  15250\n",
      "                good  14728\n",
      "                 bad  14726\n",
      "               would  14007\n",
      "              really  12355\n",
      "                time  12354\n",
      "                 see  10753\n",
      "               story  10186\n",
      "                 get  10136\n",
      "                much  10118\n",
      "              people  9466\n",
      "                make  9354\n",
      "               could  9300\n",
      "                made  8801\n",
      "                well  8539\n",
      "               first  8352\n",
      "\n",
      "[info] Vectorizing with pretrained embeddings...\n",
      "\n",
      "=== PretrainedW2V_LogReg ===\n",
      "Accuracy: 0.8557 | F1-macro: 0.8557\n",
      "\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8534    0.8590    0.8562      5000\n",
      "           1     0.8581    0.8524    0.8552      5000\n",
      "\n",
      "    accuracy                         0.8557     10000\n",
      "   macro avg     0.8557    0.8557    0.8557     10000\n",
      "weighted avg     0.8557    0.8557    0.8557     10000\n",
      "\n",
      "\n",
      "[info] Training custom Skip-gram Word2Vec...\n",
      "\n",
      "=== SkipGramW2V_LogReg ===\n",
      "Accuracy: 0.8806 | F1-macro: 0.8806\n",
      "\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8803    0.8810    0.8806      5000\n",
      "           1     0.8809    0.8802    0.8806      5000\n",
      "\n",
      "    accuracy                         0.8806     10000\n",
      "   macro avg     0.8806    0.8806    0.8806     10000\n",
      "weighted avg     0.8806    0.8806    0.8806     10000\n",
      "\n",
      "\n",
      "[info] Training custom CBOW Word2Vec...\n",
      "\n",
      "=== CBOWW2V_LogReg ===\n",
      "Accuracy: 0.8696 | F1-macro: 0.8696\n",
      "\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8684    0.8712    0.8698      5000\n",
      "           1     0.8708    0.8680    0.8694      5000\n",
      "\n",
      "    accuracy                         0.8696     10000\n",
      "   macro avg     0.8696    0.8696    0.8696     10000\n",
      "weighted avg     0.8696    0.8696    0.8696     10000\n",
      "\n",
      "\n",
      "[info] Training custom FastText...\n",
      "\n",
      "=== FastText_LogReg ===\n",
      "Accuracy: 0.8789 | F1-macro: 0.8789\n",
      "\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8787    0.8792    0.8789      5000\n",
      "           1     0.8791    0.8786    0.8789      5000\n",
      "\n",
      "    accuracy                         0.8789     10000\n",
      "   macro avg     0.8789    0.8789    0.8789     10000\n",
      "weighted avg     0.8789    0.8789    0.8789     10000\n",
      "\n",
      "\n",
      "=== Final Comparison ===\n",
      "              Method  Accuracy  F1_macro\n",
      "  SkipGramW2V_LogReg    0.8806  0.880600\n",
      "     FastText_LogReg    0.8789  0.878900\n",
      "      CBOWW2V_LogReg    0.8696  0.869600\n",
      "PretrainedW2V_LogReg    0.8557  0.855698\n",
      "[info] Saved scores and summary under: artifacts\n",
      "\n",
      "\n",
      "[info] Saved full untruncated Part 2 log to: C:\\Users\\jaypr\\Desktop\\NLP_ASS\\artifacts\\part2_results.txt\n"
     ]
    }
   ],
   "source": [
    "# ==== PART 2 ONLY: IMDB sentiment — print ALL results & save full log (with fixes) ====\n",
    "import os, io, sys, re, json, string, warnings\n",
    "from pathlib import Path\n",
    "from contextlib import redirect_stdout\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---- deps ----\n",
    "import gensim.downloader as api\n",
    "from gensim.models import Word2Vec, FastText\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "\n",
    "# --- NLTK data (minimal) ---\n",
    "for pkg, path in [(\"punkt\",\"tokenizers/punkt\"), (\"punkt_tab\",\"tokenizers/punkt_tab\"), (\"stopwords\",\"corpora/stopwords\")]:\n",
    "    try:\n",
    "        nltk.data.find(path)\n",
    "    except LookupError:\n",
    "        nltk.download(pkg, quiet=True)\n",
    "\n",
    "STOPWORDS = set(stopwords.words(\"english\"))\n",
    "RANDOM_STATE = 42\n",
    "ARTIFACT_DIR = Path(\"artifacts\"); ARTIFACT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ----------------------------- helpers & pipeline -----------------------------\n",
    "RE_HTML = re.compile(r\"<.*?>\")\n",
    "RE_URL  = re.compile(r\"(https?://\\S+|www\\.\\S+)\")\n",
    "RE_PUNCT = re.compile(rf\"[{re.escape(string.punctuation)}]\")\n",
    "\n",
    "def load_imdb():\n",
    "    import kagglehub\n",
    "    csv = None\n",
    "    try:\n",
    "        print(\"\\n[info] Using kagglehub to fetch IMDB dataset...\")\n",
    "        p = kagglehub.dataset_download(\"lakshmi25npathi/imdb-dataset-of-50k-movie-reviews\")\n",
    "        print(\"[info] kagglehub path:\", p)\n",
    "        base = Path(p)\n",
    "        cands = list(base.glob(\"*.csv\")) + list(base.rglob(\"*.csv\"))\n",
    "        for c in cands:\n",
    "            if c.name.lower().startswith(\"imdb\") and c.suffix.lower()==\".csv\":\n",
    "                csv = c; break\n",
    "        if csv is None and cands:\n",
    "            csv = cands[0]\n",
    "    except Exception as e:\n",
    "        print(\"[warn] kagglehub unavailable/failure:\", repr(e))\n",
    "        base = Path(\".\")\n",
    "        for c in list(base.glob(\"*.csv\")) + list(base.rglob(\"*.csv\")):\n",
    "            if \"imdb\" in c.name.lower():\n",
    "                csv = c; break\n",
    "    if csv is None:\n",
    "        raise FileNotFoundError(\"Could not locate IMDB CSV. Ensure it's downloaded or available locally.\")\n",
    "\n",
    "    print(f\"[info] Loading dataset: {csv}\")\n",
    "    df = pd.read_csv(csv)\n",
    "    cols = {c.lower(): c for c in df.columns}\n",
    "    review_col = cols.get(\"review\"); sent_col = cols.get(\"sentiment\")\n",
    "    if review_col is None or sent_col is None:\n",
    "        df = df.iloc[:, :2]; df.columns = [\"review\",\"sentiment\"]; review_col=\"review\"; sent_col=\"sentiment\"\n",
    "\n",
    "    df = df[[review_col, sent_col]].rename(columns={review_col:\"review\", sent_col:\"sentiment\"})\n",
    "    # >>> FIXED LINE: use .str.strip() on a Series\n",
    "    df[\"sentiment\"] = df[\"sentiment\"].astype(str).str.strip().str.lower()\n",
    "\n",
    "    mapping = {\"positive\":1, \"pos\":1, \"1\":1, \"negative\":0, \"neg\":0, \"0\":0}\n",
    "    df[\"label\"] = df[\"sentiment\"].map(mapping)\n",
    "    if df[\"label\"].isna().any():\n",
    "        df.loc[df[\"label\"].isna(),\"label\"] = (df.loc[df[\"label\"].isna(),\"sentiment\"].str[0].str.lower()==\"p\").astype(int)\n",
    "    df[\"label\"] = df[\"label\"].astype(int)\n",
    "    df.drop(columns=[\"sentiment\"], inplace=True)\n",
    "    print(f\"[info] Dataset size: {len(df)}; positives={int(df['label'].sum())}; negatives={len(df)-int(df['label'].sum())}\")\n",
    "    return df\n",
    "\n",
    "def clean_and_tokenize(text, remove_punct=True, remove_stop=True, lowercase=True):\n",
    "    text = RE_HTML.sub(\" \", str(text))\n",
    "    text = RE_URL.sub(\" \", text)\n",
    "    if lowercase: text = text.lower()\n",
    "    if remove_punct: text = RE_PUNCT.sub(\" \", text)\n",
    "    toks = word_tokenize(text)\n",
    "    if remove_stop: toks = [t for t in toks if t not in STOPWORDS and t.strip()]\n",
    "    return toks\n",
    "\n",
    "def text_eda(df):\n",
    "    print(\"\\n[info] Running EDA...\")\n",
    "    df[\"tokens\"] = df[\"review\"].apply(clean_and_tokenize)\n",
    "    df[\"length\"] = df[\"tokens\"].apply(len)\n",
    "\n",
    "    stats = df[\"length\"].describe()\n",
    "    print(\"\\nToken length summary:\"); print(stats.to_string())\n",
    "\n",
    "    plt.figure(figsize=(8,5))\n",
    "    plt.hist(df[\"length\"], bins=50)\n",
    "    plt.title(\"Review Length (tokens)\"); plt.xlabel(\"Tokens\"); plt.ylabel(\"Count\")\n",
    "    plt.tight_layout()\n",
    "    out_hist = ARTIFACT_DIR / \"imdb_length_hist.png\"\n",
    "    plt.savefig(out_hist, dpi=150); plt.close()\n",
    "    print(f\"[info] Saved histogram: {out_hist}\")\n",
    "\n",
    "    for label, name in [(1,\"positive\"), (0,\"negative\")]:\n",
    "        all_toks=[]; [all_toks.extend(t) for t in df.loc[df[\"label\"]==label, \"tokens\"]]\n",
    "        top20 = Counter(all_toks).most_common(20)\n",
    "        print(f\"\\nTop tokens ({name}):\")\n",
    "        for t,c in top20: print(f\"{t:>20s}  {c}\")\n",
    "        pd.DataFrame(top20, columns=[\"token\",\"count\"]).to_csv(ARTIFACT_DIR/f\"top_tokens_{name}.csv\", index=False)\n",
    "    return df\n",
    "\n",
    "def build_doc_vectors(token_seqs, kv, vector_size):\n",
    "    mat = np.zeros((len(token_seqs), vector_size), dtype=np.float32)\n",
    "    for i, toks in enumerate(token_seqs):\n",
    "        vs = [kv.get_vector(t) for t in toks if t in kv.key_to_index]\n",
    "        if vs: mat[i] = np.mean(vs, axis=0)\n",
    "    return mat\n",
    "\n",
    "def train_and_eval(name, X_train_vec, X_test_vec, y_train, y_test):\n",
    "    clf = LogisticRegression(max_iter=1000, solver=\"liblinear\", random_state=RANDOM_STATE)\n",
    "    clf.fit(X_train_vec, y_train)\n",
    "    preds = clf.predict(X_test_vec)\n",
    "    acc = accuracy_score(y_test, preds)\n",
    "    f1m = f1_score(y_test, preds, average=\"macro\")\n",
    "    print(f\"\\n=== {name} ===\"); print(f\"Accuracy: {acc:.4f} | F1-macro: {f1m:.4f}\")\n",
    "    rep = classification_report(y_test, preds, digits=4)\n",
    "    print(\"\\nClassification report:\\n\", rep)\n",
    "    with open(ARTIFACT_DIR / f\"classification_report_{name.replace(' ','_').lower()}.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"{name}\\nAccuracy: {acc:.6f}\\nF1-macro: {f1m:.6f}\\n\\n\"); f.write(rep)\n",
    "    return acc, f1m\n",
    "\n",
    "def part2_pipeline(kv_pretrained):\n",
    "    df = load_imdb()\n",
    "    df = text_eda(df)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        df[\"tokens\"].values, df[\"label\"].values, test_size=0.2, stratify=df[\"label\"].values, random_state=RANDOM_STATE\n",
    "    )\n",
    "    X_train_tokens, X_test_tokens = list(X_train), list(X_test)\n",
    "\n",
    "    results = []\n",
    "\n",
    "    # (1) Pretrained W2V avg-pooled\n",
    "    print(\"\\n[info] Vectorizing with pretrained embeddings...\")\n",
    "    d = kv_pretrained.vector_size\n",
    "    Xtr = build_doc_vectors(X_train_tokens, kv_pretrained, d)\n",
    "    Xte = build_doc_vectors(X_test_tokens, kv_pretrained, d)\n",
    "    acc, f1m = train_and_eval(\"PretrainedW2V_LogReg\", Xtr, Xte, y_train, y_test)\n",
    "    results.append((\"PretrainedW2V_LogReg\", acc, f1m))\n",
    "\n",
    "    # Prepare full corpus\n",
    "    sentences = list(df[\"tokens\"].values)\n",
    "\n",
    "    # (2) Skip-gram Word2Vec\n",
    "    print(\"\\n[info] Training custom Skip-gram Word2Vec...\")\n",
    "    w2v_sg = Word2Vec(sentences=sentences, vector_size=100, window=5, min_count=2,\n",
    "                      workers=4, sg=1, epochs=5, seed=RANDOM_STATE)\n",
    "    kv_sg = w2v_sg.wv\n",
    "    Xtr = build_doc_vectors(X_train_tokens, kv_sg, kv_sg.vector_size)\n",
    "    Xte = build_doc_vectors(X_test_tokens, kv_sg, kv_sg.vector_size)\n",
    "    acc, f1m = train_and_eval(\"SkipGramW2V_LogReg\", Xtr, Xte, y_train, y_test)\n",
    "    results.append((\"SkipGramW2V_LogReg\", acc, f1m))\n",
    "\n",
    "    # (3) CBOW Word2Vec\n",
    "    print(\"\\n[info] Training custom CBOW Word2Vec...\")\n",
    "    w2v_cbow = Word2Vec(sentences=sentences, vector_size=100, window=5, min_count=2,\n",
    "                        workers=4, sg=0, epochs=5, seed=RANDOM_STATE)\n",
    "    kv_cbow = w2v_cbow.wv\n",
    "    Xtr = build_doc_vectors(X_train_tokens, kv_cbow, kv_cbow.vector_size)\n",
    "    Xte = build_doc_vectors(X_test_tokens, kv_cbow, kv_cbow.vector_size)\n",
    "    acc, f1m = train_and_eval(\"CBOWW2V_LogReg\", Xtr, Xte, y_train, y_test)\n",
    "    results.append((\"CBOWW2V_LogReg\", acc, f1m))\n",
    "\n",
    "    # (4) FastText (gensim 4.x API FIX: use corpus_iterable=)\n",
    "    print(\"\\n[info] Training custom FastText...\")\n",
    "    ft = FastText(vector_size=100, window=5, min_count=2, workers=4, sg=1, seed=RANDOM_STATE)\n",
    "    try:\n",
    "        ft.build_vocab(corpus_iterable=sentences)\n",
    "        ft.train(corpus_iterable=sentences, total_examples=len(sentences), epochs=5)\n",
    "    except TypeError:\n",
    "        ft.build_vocab(sentences=sentences)\n",
    "        ft.train(sentences=sentences, total_examples=len(sentences), epochs=5)\n",
    "    kv_ft = ft.wv\n",
    "    Xtr = build_doc_vectors(X_train_tokens, kv_ft, kv_ft.vector_size)\n",
    "    Xte = build_doc_vectors(X_test_tokens, kv_ft, kv_ft.vector_size)\n",
    "    acc, f1m = train_and_eval(\"FastText_LogReg\", Xtr, Xte, y_train, y_test)\n",
    "    results.append((\"FastText_LogReg\", acc, f1m))\n",
    "\n",
    "    # Final table\n",
    "    res_df = pd.DataFrame(results, columns=[\"Method\", \"Accuracy\", \"F1_macro\"]).sort_values(\"F1_macro\", ascending=False)\n",
    "    print(\"\\n=== Final Comparison ===\"); print(res_df.to_string(index=False))\n",
    "    res_df.to_csv(ARTIFACT_DIR / \"imdb_wordvectors_scores.csv\", index=False)\n",
    "    with open(ARTIFACT_DIR / \"imdb_wordvectors_summary.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump({\"results\": [{\"method\": m, \"accuracy\": float(a), \"f1_macro\": float(f)} for m,a,f in results]}, f, indent=2)\n",
    "    print(f\"[info] Saved scores and summary under: {ARTIFACT_DIR}\")\n",
    "\n",
    "# ----------------------------- run & capture full stdout -----------------------------\n",
    "# Re-use a pretrained kv from another cell if defined; otherwise load a small fallback\n",
    "kv = globals().get(\"kv\", None)\n",
    "if kv is None:\n",
    "    try:\n",
    "        print(\"[info] Loading 'word2vec-google-news-300' (large, may be cached)...\")\n",
    "        kv = api.load(\"word2vec-google-news-300\")\n",
    "    except Exception:\n",
    "        print(\"[warn] Falling back to 'glove-wiki-gigaword-100' (smaller)\")\n",
    "        kv = api.load(\"glove-wiki-gigaword-100\")\n",
    "\n",
    "buf = io.StringIO()\n",
    "log_path = ARTIFACT_DIR / \"part2_results.txt\"\n",
    "with redirect_stdout(buf):\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    part2_pipeline(kv_pretrained=kv)\n",
    "\n",
    "print(buf.getvalue())\n",
    "with open(log_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(buf.getvalue())\n",
    "print(f\"\\n[info] Saved full untruncated Part 2 log to: {log_path.resolve()}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
