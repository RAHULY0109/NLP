{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":66653,"databundleVersionId":7500999,"sourceType":"competition"}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\"\"\"\nPII NER (BIO) with DeBERTa on Kaggle 'pii-detection-removal-from-educational-data' (FAST)\n\nZero-arg Kaggle usage:\n----------------------\npython train_pii_ner.py\n\nOptional:\n- PII_DATA_PATH=/custom/path to override auto-detect.\n- Inference:\n  python train_pii_ner.py --mode infer --text \"Email me at alice@school.edu\"\n\nSpeed notes:\n- Fused AdamW, TF32, fp16, group_by_length, persistent workers, pad_to_multiple_of=8, parallel tokenization.\n- Use --compile on A100 for extra speed after the first-epoch compile warmup.\n\"\"\"\n\nimport os\nimport io\nimport re\nimport gc\nimport json\nimport zipfile\nimport argparse\nimport random\nimport tempfile\nfrom typing import List, Dict, Any, Tuple\n\nimport numpy as np\nfrom datasets import Dataset\nfrom transformers import (\n    AutoTokenizer,\n    AutoConfig,\n    AutoModelForTokenClassification,\n    DebertaV2ForTokenClassification,   # direct class (safe path for DeBERTa v2/v3)\n    DataCollatorForTokenClassification,\n    TrainingArguments,\n    Trainer,\n    EarlyStoppingCallback,\n    set_seed,\n    pipeline as hf_pipeline,\n)\nimport torch\nfrom seqeval.metrics import precision_score, recall_score, f1_score, accuracy_score\n\n# Quieter + faster math on Ampere+\nos.environ.setdefault(\"TOKENIZERS_PARALLELISM\", \"false\")\ntry:\n    if torch.cuda.is_available():\n        torch.backends.cuda.matmul.allow_tf32 = True\n        torch.backends.cudnn.allow_tf32 = True\n    torch.set_float32_matmul_precision(\"high\")\nexcept Exception:\n    pass\n\n\n# --------------------------------\n# Kaggle-friendly auto path finder\n# --------------------------------\ndef auto_find_data_path() -> str:\n    hint = os.environ.get(\"PII_DATA_PATH\", \"\").strip()\n    if hint and os.path.exists(hint):\n        return hint\n\n    common = \"/kaggle/input/pii-detection-removal-from-educational-data\"\n    if os.path.isdir(common) and os.path.exists(os.path.join(common, \"train.json\")):\n        return common\n\n    input_root = \"/kaggle/input\"\n    if os.path.isdir(input_root):\n        for root, dirs, files in os.walk(input_root):\n            if \"train.json\" in files:\n                return root\n        for root, _, files in os.walk(input_root):\n            for f in files:\n                if f.lower().endswith(\".zip\"):\n                    zpath = os.path.join(root, f)\n                    try:\n                        with zipfile.ZipFile(zpath) as z:\n                            if any(os.path.basename(n) == \"train.json\" for n in z.namelist()):\n                                return zpath\n                    except zipfile.BadZipFile:\n                        pass\n\n    if os.path.exists(\"train.json\"):\n        return os.getcwd()\n\n    raise FileNotFoundError(\n        \"Could not auto-locate 'train.json'. \"\n        \"Attach the Kaggle dataset or set PII_DATA_PATH=/path/to/folder_or_zip.\"\n    )\n\n\ndef resolve_data_paths(data_path: str) -> Tuple[str, str]:\n    if os.path.isdir(data_path):\n        train_json = os.path.join(data_path, \"train.json\")\n        test_json = os.path.join(data_path, \"test.json\")\n        if not os.path.isfile(train_json):\n            raise FileNotFoundError(f\"train.json not found under {data_path}\")\n        return train_json, test_json if os.path.isfile(test_json) else None\n\n    if zipfile.is_zipfile(data_path):\n        td = tempfile.mkdtemp(prefix=\"pii_zip_\")\n        with zipfile.ZipFile(data_path) as z:\n            z.extractall(td)\n        train_json = os.path.join(td, \"train.json\")\n        if os.path.isfile(train_json):\n            test_json = os.path.join(td, \"test.json\")\n            return train_json, test_json if os.path.isfile(test_json) else None\n        for root, _, files in os.walk(td):\n            if \"train.json\" in files:\n                test_json = os.path.join(root, \"test.json\") if \"test.json\" in files else None\n                return os.path.join(root, \"train.json\"), test_json\n\n    raise ValueError(f\"Invalid data_path: {data_path}\")\n\n\n# -----------------------------\n# JSON loading and HF datasets\n# -----------------------------\ndef load_json_records(path: str) -> List[Dict[str, Any]]:\n    with io.open(path, \"r\", encoding=\"utf-8\") as f:\n        obj = json.load(f)\n    if isinstance(obj, dict):\n        if all(isinstance(v, dict) for v in obj.values()):\n            records = list(obj.values())\n        elif \"data\" in obj and isinstance(obj[\"data\"], list):\n            records = obj[\"data\"]\n        else:\n            raise ValueError(\"Unexpected JSON structure; expected list or dict-of-records.\")\n    elif isinstance(obj, list):\n        records = obj\n    else:\n        raise ValueError(\"Unexpected JSON structure; expected list or dict.\")\n    for i, r in enumerate(records):\n        if \"tokens\" not in r:\n            raise ValueError(f\"Missing 'tokens' in record {i}\")\n        if \"labels\" in r and len(r[\"labels\"]) != len(r[\"tokens\"]):\n            raise ValueError(f\"labels length != tokens length at record {i}\")\n    return records\n\n\ndef records_to_hf_dataset(records: List[Dict[str, Any]], with_labels: bool = True) -> Dataset:\n    data = {\n        \"tokens\": [r[\"tokens\"] for r in records],\n        \"trailing_whitespace\": [r.get(\"trailing_whitespace\", [True] * len(r[\"tokens\"])) for r in records],\n        \"document\": [r.get(\"document\", -1) for r in records],\n        \"full_text\": [r.get(\"full_text\", \"\") for r in records],\n    }\n    if with_labels:\n        data[\"ner_tags_str\"] = [r[\"labels\"] for r in records]\n    return Dataset.from_dict(data)\n\n\ndef build_label_list(train_records: List[Dict[str, Any]]) -> List[str]:\n    uniq = set()\n    for r in train_records:\n        if \"labels\" in r and r[\"labels\"] is not None:\n            uniq.update(r[\"labels\"])\n    uniq.discard(\"O\")\n    b_tags = sorted([x for x in uniq if x.startswith(\"B-\")])\n    i_tags = sorted([x for x in uniq if x.startswith(\"I-\")])\n    return [\"O\"] + b_tags + i_tags\n\n\n# -----------------------------\n# Tokenization & BIO alignment\n# -----------------------------\ndef tokenize_and_align_labels_fast(\n    examples: Dict[str, Any],\n    tokenizer: AutoTokenizer,\n    label2id: Dict[str, int],\n    max_length: int,\n    doc_stride: int,\n) -> Dict[str, Any]:\n    tokenized = tokenizer(\n        examples[\"tokens\"],\n        is_split_into_words=True,\n        truncation=True,\n        padding=False,\n        max_length=max_length,\n        stride=doc_stride,\n        return_overflowing_tokens=True,\n        return_offsets_mapping=False,\n    )\n\n    if \"ner_tags_str\" not in examples:\n        return tokenized\n\n    all_labels = []\n    overflow_to_sample = tokenized.pop(\"overflow_to_sample_mapping\")\n    for i in range(len(overflow_to_sample)):\n        sample_idx = overflow_to_sample[i]\n        word_ids = tokenized.word_ids(i)\n        word_labels = examples[\"ner_tags_str\"][sample_idx]\n\n        labels = []\n        prev_wid = None\n        for wid in word_ids:\n            if wid is None:\n                labels.append(-100)\n            else:\n                if wid != prev_wid:\n                    labels.append(label2id.get(word_labels[wid], label2id[\"O\"]))  # first subtoken -> label\n                else:\n                    labels.append(-100)  # subsequent subtokens ignored\n                prev_wid = wid\n        all_labels.append(labels)\n\n    tokenized[\"labels\"] = all_labels\n    return tokenized\n\n\n# -----------------------------\n# Metrics (seqeval)\n# -----------------------------\ndef compute_seqeval_metrics(p: Any, id2label: Dict[int, str]) -> Dict[str, float]:\n    preds = np.argmax(p.predictions, axis=-1)\n    labels = p.label_ids\n\n    true_labels = []\n    true_preds = []\n    for pred_row, lab_row in zip(preds, labels):\n        y_true, y_pred = [], []\n        for p_i, l_i in zip(pred_row, lab_row):\n            if l_i == -100:\n                continue\n            y_true.append(id2label[int(l_i)])\n            y_pred.append(id2label[int(p_i)])\n        true_labels.append(y_true)\n        true_preds.append(y_pred)\n\n    return {\n        \"precision\": precision_score(true_labels, true_preds),\n        \"recall\": recall_score(true_labels, true_preds),\n        \"f1\": f1_score(true_labels, true_preds),\n        \"accuracy\": accuracy_score(true_labels, true_preds),\n    }\n\n\n# -----------------------------\n# Train/val split\n# -----------------------------\ndef build_datasets(train_json: str, val_ratio: float, seed: int):\n    records = load_json_records(train_json)\n    rng = random.Random(seed)\n    rng.shuffle(records)\n    n = len(records)\n    n_val = max(1, int(n * val_ratio))\n    val_records = records[:n_val]\n    train_records = records[n_val:]\n    return (\n        records_to_hf_dataset(train_records, with_labels=True),\n        records_to_hf_dataset(val_records, with_labels=True),\n        train_records,\n    )\n\n\n# -----------------------------\n# Main\n# -----------------------------\ndef main():\n    parser = argparse.ArgumentParser(description=\"DeBERTa NER (BIO) for Kaggle PII Detection â€” FAST\")\n    parser.add_argument(\"--mode\", type=str, default=\"train\", choices=[\"train\", \"infer\"])\n    parser.add_argument(\"--data-path\", type=str, default=\"\", help=\"Leave empty on Kaggle; auto-detects /kaggle/input/**\")\n    parser.add_argument(\"--out-dir\", type=str, default=\"/kaggle/working/pii_deberta_v3_base\")\n    parser.add_argument(\"--model-name\", type=str, default=\"microsoft/deberta-v3-base\")\n    parser.add_argument(\"--epochs\", type=int, default=3)\n    parser.add_argument(\"--lr\", type=float, default=2e-5)\n    parser.add_argument(\"--weight-decay\", type=float, default=0.01)\n    parser.add_argument(\"--warmup-ratio\", type=float, default=0.1)\n    parser.add_argument(\"--train-batch\", type=int, default=8)\n    parser.add_argument(\"--eval-batch\", type=int, default=16)\n    parser.add_argument(\"--gradient-accumulation\", type=int, default=1)\n    parser.add_argument(\"--max-length\", type=int, default=512)\n    parser.add_argument(\"--doc-stride\", type=int, default=128)\n    parser.add_argument(\"--val-ratio\", type=float, default=0.1)\n    parser.add_argument(\"--seed\", type=int, default=42)\n    parser.add_argument(\"--fp16\", action=\"store_true\", help=\"Enable FP16 mixed precision\")\n    parser.add_argument(\"--bf16\", action=\"store_true\", help=\"Enable BF16 mixed precision\")\n    parser.add_argument(\"--grad-checkpointing\", action=\"store_true\")\n    parser.add_argument(\"--dataloader-workers\", type=int, default=4)\n    parser.add_argument(\"--log-steps\", type=int, default=50)\n    parser.add_argument(\"--patience\", type=int, default=2)\n    parser.add_argument(\"--save-best-only\", action=\"store_true\")\n    parser.add_argument(\"--compile\", action=\"store_true\", help=\"Use torch.compile (A100 recommended)\")\n    parser.add_argument(\"--text\", type=str, default=\"\")\n    args, _ = parser.parse_known_args()\n\n    # Default to fp16 on GPU unless bf16 requested\n    if torch.cuda.is_available() and not args.bf16:\n        args.fp16 = True\n\n    set_seed(args.seed)\n    os.makedirs(args.out_dir, exist_ok=True)\n\n    if args.mode == \"infer\":\n        print(f\"[info] Loading model from {args.out_dir} ...\")\n        tokenizer = AutoTokenizer.from_pretrained(args.out_dir, use_fast=True)\n        try:\n            model = AutoModelForTokenClassification.from_pretrained(args.out_dir)\n        except (ModuleNotFoundError, ImportError):\n            model = DebertaV2ForTokenClassification.from_pretrained(args.out_dir)\n        nlp = hf_pipeline(\n            \"token-classification\",\n            model=model,\n            tokenizer=tokenizer,\n            aggregation_strategy=\"simple\",\n            device=0 if torch.cuda.is_available() else -1,\n        )\n        text = args.text.strip() or \"Email me at alice_01@school.edu or call 555-123-4567. I'm Alice from 221B Baker Street.\"\n        print(\"[demo] Input:\", text)\n        print(\"[demo] Aggregated entities:\", nlp(text))\n        return\n\n    # --------------------- TRAIN MODE ---------------------\n    data_path = args.data_path.strip() or auto_find_data_path()\n    print(f\"[info] Using data path: {data_path}\")\n    train_json, _ = resolve_data_paths(data_path)\n\n    train_ds, val_ds, train_records = build_datasets(train_json, args.val_ratio, args.seed)\n\n    # Labels\n    label_list = build_label_list(train_records)\n    id2label = {i: l for i, l in enumerate(label_list)}\n    label2id = {l: i for i, l in enumerate(label_list)}\n    print(\"[info] Labels:\", label_list)\n\n    # Tokenizer\n    tokenizer = AutoTokenizer.from_pretrained(args.model_name, use_fast=True)\n\n    # Parallel map/tokenize with overflow windows + BIO alignment\n    NUM_PROC = max(1, min(4, (os.cpu_count() or 2)))\n    def _map_fn(batch):\n        return tokenize_and_align_labels_fast(\n            batch, tokenizer, label2id, max_length=args.max_length, doc_stride=args.doc_stride\n        )\n\n    train_tok = train_ds.map(\n        _map_fn, batched=True, num_proc=NUM_PROC,\n        remove_columns=train_ds.column_names, desc=f\"Tokenizing train (num_proc={NUM_PROC})\"\n    )\n    val_tok = val_ds.map(\n        _map_fn, batched=True, num_proc=NUM_PROC,\n        remove_columns=val_ds.column_names, desc=f\"Tokenizing val (num_proc={NUM_PROC})\"\n    )\n\n    # Model config\n    config = AutoConfig.from_pretrained(\n        args.model_name,\n        num_labels=len(label_list),\n        id2label=id2label,\n        label2id=label2id,\n    )\n\n    # Robust model loader that avoids Auto's wide import scan (glm, etc.)\n    def load_model(model_name: str, cfg: AutoConfig):\n        name_l = model_name.lower()\n        if \"deberta-v2\" in name_l or \"deberta-v3\" in name_l or \"deberta\" in name_l:\n            return DebertaV2ForTokenClassification.from_pretrained(model_name, config=cfg)\n        return AutoModelForTokenClassification.from_pretrained(model_name, config=cfg)\n\n    try:\n        model = load_model(args.model_name, config)\n    except (ModuleNotFoundError, ImportError):\n        model = DebertaV2ForTokenClassification.from_pretrained(args.model_name, config=config)\n    except OSError as e:\n        raise OSError(\n            f\"Failed to load {args.model_name}. On Kaggle (no internet), make sure the model is cached \"\n            f\"or attach it as a dataset / enable internet for first run.\"\n        ) from e\n\n    if args.grad_checkpointing:\n        model.gradient_checkpointing_enable()\n    model.config.use_cache = False\n\n    # Collator: dynamic padding, align to multiple of 8 to use Tensor Cores\n    collator = DataCollatorForTokenClassification(tokenizer=tokenizer, pad_to_multiple_of=8)\n\n    # TrainingArguments (FAST)\n    training_args = TrainingArguments(\n        output_dir=args.out_dir,\n        eval_strategy=\"epoch\",      # <- fix: must be evaluation_strategy\n        save_strategy=\"epoch\",\n        logging_strategy=\"steps\",\n        logging_steps=args.log_steps,\n\n        per_device_train_batch_size=args.train_batch,\n        per_device_eval_batch_size=args.eval_batch,\n        gradient_accumulation_steps=args.gradient_accumulation,\n\n        num_train_epochs=args.epochs,\n        learning_rate=args.lr,\n        weight_decay=args.weight_decay,\n        warmup_ratio=args.warmup_ratio,\n        lr_scheduler_type=\"linear\",\n\n        load_best_model_at_end=True,\n        metric_for_best_model=\"f1\",\n        greater_is_better=True,\n\n        # SPEED knobs\n        optim=\"adamw_torch_fused\" if torch.cuda.is_available() else \"adamw_torch\",\n        group_by_length=True,                    # less padding -> faster\n        dataloader_num_workers=args.dataloader_workers,\n        dataloader_pin_memory=True,\n        dataloader_persistent_workers=True,\n        fp16=args.fp16,\n        bf16=args.bf16,\n        fp16_full_eval=args.fp16,               # eval with AMP too\n        eval_accumulation_steps=64,             # lower GPU mem spikes during eval\n        torch_compile=args.compile,             # use --compile to enable\n        save_total_limit=1 if args.save_best_only else 3,\n        save_safetensors=True,\n        report_to=\"none\",\n        seed=args.seed,\n    )\n\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_tok,\n        eval_dataset=val_tok,\n        data_collator=collator,\n        tokenizer=tokenizer,\n        compute_metrics=lambda p: compute_seqeval_metrics(p, id2label),\n        callbacks=[EarlyStoppingCallback(early_stopping_patience=args.patience)],\n    )\n\n    print(\"[info] Starting training...\")\n    trainer.train()\n    metrics = trainer.evaluate()\n    print(\"[eval] metrics:\", metrics)\n\n    print(\"[info] Saving to\", args.out_dir)\n    trainer.save_model(args.out_dir)\n    tokenizer.save_pretrained(args.out_dir)\n\n    # Quick demo with aggregation\n    try:\n        nlp = hf_pipeline(\n            \"token-classification\",\n            model=trainer.model,\n            tokenizer=tokenizer,\n            aggregation_strategy=\"simple\",\n            device=0 if torch.cuda.is_available() else -1,\n        )\n        sample = val_ds[0]\n        toks = sample[\"tokens\"]\n        ws = sample[\"trailing_whitespace\"]\n        text = \"\".join([t + (\" \" if (i < len(ws) and ws[i]) else \"\") for i, t in enumerate(toks)])\n        text = re.sub(r\"\\s+\", \" \", text).strip()\n        print(\"[demo] Aggregated NER on a val sample:\", nlp(text))\n    except Exception as e:\n        print(f\"[warn] Demo pipeline failed: {e}\")\n\n    del trainer, model\n    gc.collect()\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-07T13:38:41.852613Z","iopub.execute_input":"2025-09-07T13:38:41.853211Z","iopub.status.idle":"2025-09-07T14:18:17.939915Z","shell.execute_reply.started":"2025-09-07T13:38:41.853176Z","shell.execute_reply":"2025-09-07T14:18:17.938944Z"}},"outputs":[{"name":"stdout","text":"[info] Using data path: /kaggle/input/pii-detection-removal-from-educational-data\n[info] Labels: ['O', 'B-EMAIL', 'B-ID_NUM', 'B-NAME_STUDENT', 'B-PHONE_NUM', 'B-STREET_ADDRESS', 'B-URL_PERSONAL', 'B-USERNAME', 'I-ID_NUM', 'I-NAME_STUDENT', 'I-PHONE_NUM', 'I-STREET_ADDRESS', 'I-URL_PERSONAL']\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"39c8f0c5fb1549e6992c969c0340e375"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/579 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a75843144f5d4314bc3fe1a27fb1f033"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spm.model:   0%|          | 0.00/2.46M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c90cb64a2a4e46c49f56c4eb12871985"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Tokenizing train (num_proc=4) (num_proc=4):   0%|          | 0/6127 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eb493a5b76d7489dbaed582d09d870ca"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Tokenizing val (num_proc=4) (num_proc=4):   0%|          | 0/680 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ef9add2e6ce441ed91b8048741625cdb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/371M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b506fe3c1da84a9ea7bc12bbbf238b11"}},"metadata":{}},{"name":"stderr","text":"Some weights of DebertaV2ForTokenClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/tmp/ipykernel_36/2518578234.py:421: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n","output_type":"stream"},{"name":"stdout","text":"[info] Starting training...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/371M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ac1eb58da6204853a1a5da59e3c69fa7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='4551' max='4551' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [4551/4551 38:16, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.000500</td>\n      <td>0.000503</td>\n      <td>0.846154</td>\n      <td>0.814815</td>\n      <td>0.830189</td>\n      <td>0.999854</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.000300</td>\n      <td>0.000378</td>\n      <td>0.775229</td>\n      <td>0.894180</td>\n      <td>0.830467</td>\n      <td>0.999851</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.000100</td>\n      <td>0.000395</td>\n      <td>0.890710</td>\n      <td>0.862434</td>\n      <td>0.876344</td>\n      <td>0.999890</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='87' max='87' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [87/87 00:27]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"[eval] metrics: {'eval_loss': 0.0003909986699000001, 'eval_precision': 0.8907103825136612, 'eval_recall': 0.8624338624338624, 'eval_f1': 0.8763440860215054, 'eval_accuracy': 0.9998897342595655, 'eval_runtime': 30.3462, 'eval_samples_per_second': 45.805, 'eval_steps_per_second': 2.867, 'epoch': 3.0}\n[info] Saving to /kaggle/working/pii_deberta_v3_base\n","output_type":"stream"},{"name":"stderr","text":"Device set to use cuda:0\nAsking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n","output_type":"stream"},{"name":"stdout","text":"[demo] Aggregated NER on a val sample: []\n","output_type":"stream"}],"execution_count":3}]}